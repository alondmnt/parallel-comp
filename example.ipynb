{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARASCHUT notebook\n",
    "\n",
    "we'll go through a small-scale example of `parachut` tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example/job_queue.db\n",
      "example/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paraschut as psu\n",
    "print(psu.config.QFile)\n",
    "print(psu.config.JobDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate a job\n",
    "\n",
    "we'll start from the default template and update it with data relevant to our example. note that these functions may be run offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BatchID': 20210225224111,\n",
       " 'JobIndex': 0,\n",
       " 'priority': 1,\n",
       " 'name': 'example',\n",
       " 'batch_type': 'foo',\n",
       " 'data': None,\n",
       " 'script': 'python example/job.py {BatchID} {JobIndex}',\n",
       " 'queue': None,\n",
       " 'resources': None,\n",
       " 'state': 'init',\n",
       " 'CodeDir': '/home/ec2-user/tools/parallel-comp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobinfo = psu.get_job_template(SetID=True)\n",
    "jobinfo['name'] = 'example'\n",
    "jobinfo['CodeDir'] = os.path.abspath('.')\n",
    "jobinfo['JobIndex'] = 0\n",
    "jobinfo['script'] = 'python example/job.py {BatchID} {JobIndex}'\n",
    "# jobinfo['script'] = 'example/template.sh'\n",
    "# jobinfo['pyfile'] = 'example/job.py'\n",
    "jobinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's add some random data for the job to operate on. this job will just output its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example/20210225224111/data_0.pkl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "data = randint(1, 100, (1, 10**4))\n",
    "psu.generate_data(jobinfo, data)\n",
    "jobinfo['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python example/job.py 20210225224111 0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psu.generate_script(jobinfo)\n",
    "jobinfo['script']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you may also try setting the 'script' field to 'example/template.sh' and try generating a script. watch the script file that is written in this case.\n",
    "\n",
    "finally, let's add the job we built to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "psu.add_job_to_queue(jobinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's check that a new job (with JobIndex=0) was added to our queue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20210225224111: example\n",
      "{'init': [0]}\n",
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 0\n",
      "running/complete/total: 0/0/1\n"
     ]
    }
   ],
   "source": [
    "psu.get_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE, that the server queue job counter (appearing in the last line of `get_queue` output) counts all currently online jobs associated with one's user (including those that are not part of the projects currently managed using `paraschut`).\n",
    "\n",
    "next, let's verify that the metadata has been properly stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BatchID': 20210225224111,\n",
       " 'JobIndex': 0,\n",
       " 'priority': 1,\n",
       " 'name': 'example',\n",
       " 'batch_type': 'foo',\n",
       " 'data': 'example/20210225224111/data_0.pkl',\n",
       " 'script': 'python example/job.py 20210225224111 0',\n",
       " 'queue': None,\n",
       " 'resources': None,\n",
       " 'state': 'init',\n",
       " 'CodeDir': '/home/ec2-user/tools/parallel-comp',\n",
       " 'md5': 'f4005066d30f34ec323850f0954a3536'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psu.get_job_info(20210225224111, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple jobs and collection\n",
    "\n",
    "first, we'll add 3 more simlar jobs similar to our first job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_job(jobinfo, i):\n",
    "    newjob = jobinfo.copy()  # duplicating to keep BatchID and similar fields identical\n",
    "    newjob['script'] = 'python example/job.py {BatchID} {JobIndex}'\n",
    "#     newjob['script'] = 'example/template.sh'\n",
    "    newjob['JobIndex'] = i\n",
    "\n",
    "    data = randint(1, 100, (1, 10**4))\n",
    "    psu.generate_data(newjob, data)\n",
    "\n",
    "    psu.add_job_to_queue(newjob, build_script=True)\n",
    "    # this will also generate the script\n",
    "\n",
    "for i in range(3):\n",
    "    duplicate_job(jobinfo, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's verify that we indeed generated additional jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20210225224111: example\n",
      "{'init': [0, 1, 2, 3]}\n",
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 0\n",
      "running/complete/total: 0/0/4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BatchID': 20210225224111,\n",
       " 'JobIndex': 3,\n",
       " 'priority': 1,\n",
       " 'name': 'example',\n",
       " 'batch_type': 'foo',\n",
       " 'data': 'example/20210225224111/data_3.pkl',\n",
       " 'script': 'python example/job.py 20210225224111 3',\n",
       " 'queue': None,\n",
       " 'resources': None,\n",
       " 'state': 'init',\n",
       " 'CodeDir': '/home/ec2-user/tools/parallel-comp',\n",
       " 'md5': '1b27256a0e89a9e6252158bf972ed7c3'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psu.get_queue()\n",
    "psu.get_job_info(20210225224111, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, let's add a collect job that will compute the mean of means. this job will execute only once the first 4 jobs have completed successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newjob = jobinfo.copy()\n",
    "newjob['priority'] = 0.5  # lower priority gets executed after higher priority jobs are done\n",
    "newjob['script'] = 'python example/collect_job.py {BatchID} {JobIndex}'\n",
    "# newjob['script'] = 'example/template.sh'\n",
    "# newjob['pyfile'] = 'example/collect_job.py'\n",
    "newjob['JobIndex'] = 4\n",
    "newjob['data'] = range(4)  # pointing to previous JobIndices to compute the mean of their results\n",
    "\n",
    "psu.add_job_to_queue(newjob, build_script=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submit jobs\n",
    "the only job control function that must run on a server. in our case LocalJobExecutor is configured to run on the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submiting:\tpython example/job.py 20210225224111 0\n",
      "submiting:\tpython example/job.py 20210225224111 1\n",
      "submiting:\tpython example/job.py 20210225224111 2\n",
      "submiting:\tpython example/job.py 20210225224111 3\n",
      "max jobs: 1000\n",
      "in queue: 0\n",
      "submitted: 4\n"
     ]
    }
   ],
   "source": [
    "psu.submit_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that only the first 4 jobs were submitted and are currently running. the collect job is waiting for them to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monitor jobs\n",
    "\n",
    "\n",
    "let's check if the job is indeed online and running: (note the * next to jobs 0-3 in the batch, which indicates that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20210225224111: example\n",
      "{'run': ['0*', '1*', '2*', '3*'], 'init': [4]}\n",
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 4\n",
      "running/complete/total: 4/0/5\n"
     ]
    }
   ],
   "source": [
    "psu.get_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the output looks once the jobs have finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20210225224111: example\n",
      "{'complete': [0, 1, 2, 3], 'init': [4]}\n",
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 0\n",
      "running/complete/total: 0/4/5\n"
     ]
    }
   ],
   "source": [
    "psu.get_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's time to run the collect job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submiting:\tpython example/collect_job.py 20210225224111 4\n",
      "max jobs: 1000\n",
      "in queue: 0\n",
      "submitted: 1\n"
     ]
    }
   ],
   "source": [
    "psu.submit_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after a short while all jobs should be in 'complete' state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20210225224111: example\n",
      "{'complete': [0, 1, 2, 3, 4]}\n",
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 0\n",
      "running/complete/total: 0/5/5\n"
     ]
    }
   ],
   "source": [
    "psu.get_queue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now check the logs created by the jobs (stdout and sterr), and its post-run metadata (which may includs a PBS report summary, for example). in this case, the result was printed to screen in the stdout file as well as stored in the 'result' field of the job metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[[stdout log for 20210225224111/example/job_4:]]]\n",
      "\n",
      "50.183325\n",
      "max jobs: 1000\n",
      "in queue: 0\n",
      "submitted: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BatchID': 20210225224111,\n",
       " 'JobIndex': 4,\n",
       " 'priority': 0.5,\n",
       " 'name': 'example',\n",
       " 'batch_type': 'foo',\n",
       " 'data': range(0, 4),\n",
       " 'script': 'python example/collect_job.py 20210225224111 4',\n",
       " 'queue': None,\n",
       " 'resources': None,\n",
       " 'state': 'complete',\n",
       " 'CodeDir': '/home/ec2-user/tools/parallel-comp',\n",
       " 'subtime': 20210225224451,\n",
       " 'stdout': ['example/20210225224111/logs/example.o4293091116'],\n",
       " 'stderr': ['example/20210225224111/logs/example.e4293091116'],\n",
       " 'hostname': 'ip-10-217-9-184',\n",
       " 'result': 50.183325,\n",
       " 'qstat': {},\n",
       " 'md5': '227158f81a8b5caf5e3d6b1955ed08db'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psu.print_log(20210225224111, 4, 'stdout')\n",
    "psu.get_job_info(20210225224111, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we may clear all batches that have completed all their jobs using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "missing jobs: {}\n",
      "\n",
      "total jobs on server queue: 0\n",
      "running/complete/total: 0/0/0\n"
     ]
    }
   ],
   "source": [
    "psu.remove_batch_by_state('complete')\n",
    "psu.get_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}